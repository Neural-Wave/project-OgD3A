{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Notears' from 'cdt.causality.graph' (/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/cdt/causality/graph/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcdt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcausality\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PC, Notears\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load datasets\u001b[39;00m\n\u001b[1;32m      7\u001b[0m low_scrap \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/teamspace/studios/this_studio/dataset/low_scrap.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Notears' from 'cdt.causality.graph' (/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/cdt/causality/graph/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from cdt.causality.graph import PC, Notears\n",
    "\n",
    "# Load datasets\n",
    "low_scrap = pd.read_csv('/teamspace/studios/this_studio/dataset/low_scrap.csv')\n",
    "high_scrap = pd.read_csv('/teamspace/studios/this_studio/dataset/high_scrap.csv')\n",
    "\n",
    "# Automatically infer the structure based on columns\n",
    "num_features = low_scrap_df.shape[1]\n",
    "station_order = [i // 20 for i in range(num_features)]  # 5 stations, 20 features per station\n",
    "\n",
    "# Generate constraints to ensure each feature can influence only itself or subsequent stations\n",
    "def create_constraints(station_order):\n",
    "    constraints = []\n",
    "    for i in range(num_features):\n",
    "        for j in range(i + 1, num_features):\n",
    "            if station_order[i] <= station_order[j]:  # Only influence the same or subsequent stations\n",
    "                constraints.append((i, j))\n",
    "    return constraints\n",
    "\n",
    "# Get the constraints based on the station order\n",
    "constraints = create_constraints(station_order)\n",
    "\n",
    "# Use a causal discovery model like Notears, respecting the constraints\n",
    "causal_model = Notears()\n",
    "\n",
    "# Fit the model to low and high scrap datasets\n",
    "low_scrap_graph = causal_model.predict(low_scrap_df, constraints=constraints)\n",
    "high_scrap_graph = causal_model.predict(high_scrap_df, constraints=constraints)\n",
    "\n",
    "# Extract adjacency matrices for comparison\n",
    "low_scrap_adj_matrix = nx.adjacency_matrix(low_scrap_graph).todense()\n",
    "high_scrap_adj_matrix = nx.adjacency_matrix(high_scrap_graph).todense()\n",
    "\n",
    "# Save the adjacency matrix for high scrap\n",
    "np.savetxt(\"high_scrap_adj_matrix.txt\", high_scrap_adj_matrix, fmt='%d')\n",
    "\n",
    "# Rank influences on target variable 'Station5_mp_85'\n",
    "target_index = high_scrap_df.columns.get_loc(\"Station5_mp_85\")\n",
    "influences = high_scrap_adj_matrix[:, target_index].flatten()\n",
    "\n",
    "# Sort influences by strength on the target variable\n",
    "influences_sorted_indices = np.argsort(-influences)\n",
    "ranked_influences = [(high_scrap_df.columns[idx], influences[idx]) for idx in influences_sorted_indices if influences[idx] > 0]\n",
    "\n",
    "# Display top influences\n",
    "print(\"Top influences on Station5_mp_85:\", ranked_influences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Notears' from 'cdt.causality.graph' (/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/cdt/causality/graph/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcdt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcausality\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Notears\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load datasets\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Notears' from 'cdt.causality.graph' (/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/cdt/causality/graph/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from cdt.causality.graph import Notears\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load datasets\n",
    "low_scrap = pd.read_csv('/teamspace/studios/this_studio/dataset/low_scrap.csv')\n",
    "high_scrap = pd.read_csv('/teamspace/studios/this_studio/dataset/high_scrap.csv')\n",
    "\n",
    "\n",
    "# Inferring station structure\n",
    "num_features = low_scrap_df.shape[1]\n",
    "station_order = [i // 20 for i in range(num_features)]  # 5 stations, 20 measurements per station\n",
    "\n",
    "# Generate constraints to respect sequential station influence\n",
    "def create_constraints(station_order):\n",
    "    constraints = []\n",
    "    for i in range(num_features):\n",
    "        for j in range(i + 1, num_features):\n",
    "            if station_order[i] <= station_order[j]:  # Constrain to self and subsequent stations\n",
    "                constraints.append((i, j))\n",
    "    return constraints\n",
    "\n",
    "constraints = create_constraints(station_order)\n",
    "\n",
    "# Initialize causal model with constraints\n",
    "causal_model = Notears()\n",
    "high_scrap_graph = causal_model.predict(high_scrap_df, constraints=constraints)\n",
    "\n",
    "# Convert causal graph to adjacency matrix and save\n",
    "high_scrap_adj_matrix = nx.adjacency_matrix(high_scrap_graph).todense()\n",
    "np.savetxt(\"high_scrap_adj_matrix.txt\", high_scrap_adj_matrix, fmt='%d')\n",
    "\n",
    "# Calculate differences in means and variances\n",
    "mean_diffs = high_scrap_df.mean() - low_scrap_df.mean()\n",
    "var_diffs = high_scrap_df.var() - low_scrap_df.var()\n",
    "\n",
    "# Analyze influence on Station5_mp_85\n",
    "target_index = high_scrap_df.columns.get_loc(\"Station5_mp_85\")\n",
    "influences = high_scrap_adj_matrix[:, target_index].flatten()\n",
    "\n",
    "# Rank and combine structural and distributional shifts\n",
    "ranked_influences = [(high_scrap_df.columns[idx], influences[idx], mean_diffs[idx], var_diffs[idx]) \n",
    "                     for idx in np.argsort(-influences) if influences[idx] > 0]\n",
    "\n",
    "# Display top influences\n",
    "print(\"Top factors influencing Station5_mp_85:\")\n",
    "for feature, influence, mean_diff, var_diff in ranked_influences[:10]:\n",
    "    print(f\"{feature} | Influence: {influence}, Mean Diff: {mean_diff}, Var Diff: {var_diff}\")\n",
    "\n",
    "# Visualize the causal graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(high_scrap_graph, with_labels=True, font_weight='bold')\n",
    "plt.title(\"Causal Graph for High Scrap Data\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
